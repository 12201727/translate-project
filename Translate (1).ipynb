{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91HOW4MoKytt",
        "outputId": "f7c2df43-2277-4451-b46e-af31bc7668a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in d:\\osama\\anaconda3\\lib\\site-packages (23.2.1)"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: To modify pip, please run the following command:\n",
            "D:\\osama\\anaconda3\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting pip\n",
            "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/b7/3f/945ef7ab14dc4f9d7f40288d2df998d1837ee0888ec3659c813487572faa/pip-25.2-py3-none-any.whl.metadata\n",
            "  Using cached pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Using cached pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "Requirement already satisfied: typing_extensions in d:\\osama\\anaconda3\\lib\\site-packages (4.7.1)\n",
            "Collecting typing_extensions\n",
            "  Obtaining dependency information for typing_extensions from https://files.pythonhosted.org/packages/b5/00/d631e67a838026495268c2f6884f3711a15a9a2a96cd244fdaea53b823fb/typing_extensions-4.14.1-py3-none-any.whl.metadata\n",
            "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "Installing collected packages: typing_extensions\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "Successfully installed typing_extensions-4.14.1\n",
            "Requirement already satisfied: datasets in d:\\osama\\anaconda3\\lib\\site-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (1.24.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: multiprocess in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (0.15.1)\n",
            "Requirement already satisfied: packaging in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\osama\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in d:\\osama\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\osama\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\osama\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\osama\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\osama\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in d:\\osama\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in d:\\osama\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: filelock in d:\\osama\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\osama\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\osama\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\osama\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\osama\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: six in d:\\osama\\anaconda3\\lib\\site-packages (from responses<0.19->datasets) (1.16.0)\n",
            "Requirement already satisfied: colorama in d:\\osama\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\osama\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\osama\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in d:\\osama\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Collecting sentencepiece\n",
            "  Obtaining dependency information for sentencepiece from https://files.pythonhosted.org/packages/32/b8/f709977f5fda195ae1ea24f24e7c581163b6f142b1005bc3d0bbfe4d7082/sentencepiece-0.2.1-cp311-cp311-win_amd64.whl.metadata\n",
            "  Downloading sentencepiece-0.2.1-cp311-cp311-win_amd64.whl.metadata (10 kB)\n",
            "Downloading sentencepiece-0.2.1-cp311-cp311-win_amd64.whl (1.1 MB)\n",
            "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.1/1.1 MB 1.6 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 0.2/1.1 MB 1.9 MB/s eta 0:00:01\n",
            "   --------------- ------------------------ 0.4/1.1 MB 2.4 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 0.5/1.1 MB 2.4 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 0.6/1.1 MB 2.3 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 0.8/1.1 MB 2.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 1.0/1.1 MB 2.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.1/1.1 MB 2.8 MB/s eta 0:00:00\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade typing_extensions\n",
        "\n",
        "!pip install datasets\n",
        "!pip install sentencepiece\n",
        "!pip install \"transformers[torch]\"\n",
        "!pip install sacrebleu\n",
        "!pip install evaluate\n",
        "!pip install accelerate -U\n",
        "!pip install gradio\n",
        "!pip install kaleido cohere openai tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdQ1_kZGLlxa"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"cfilt/iitb-english-hindi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emdzPisnLs-q",
        "outputId": "e060605b-c9d4-4862-fc7f-c48a95f1e3b9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ],
      "source": [
        "max_length = 256\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Wo4R1ICiL8rI",
        "outputId": "9f3e9505-5f23-478f-8e1d-564e6d00bd9a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'एमएनएपी शिक्षकों के राष्ट्रपति, राजस्वीवर ने इस पुरस्कार को पेश करने के द्वारा स्कूल की प्रतिष्ठा की.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "article = dataset['validation'][2]['translation']['en']\n",
        "inputs = tokenizer(article, return_tensors=\"pt\")\n",
        "\n",
        "translated_tokens = model.generate(\n",
        "     **inputs,  max_length=256\n",
        " )\n",
        "tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v_KjbdfJMBqP",
        "outputId": "ac4d8cc8-3e69-495c-fe40-479b6058fee7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'मनपा शिक्षक संघ के अध्यक्ष राजेश गवरे ने स्कूल को भेंट देकर सराहना की।'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['validation'][2]['translation']['hi']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YFxhIFJMF4A"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "  inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
        "  targets = [ex[\"hi\"] for ex in examples[\"translation\"]]\n",
        "\n",
        "  model_inputs = tokenizer(inputs, max_length=max_length, truncation=True)\n",
        "  labels = tokenizer(targets,max_length=max_length, truncation=True)\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "  return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3GgM19fMJLw"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets_validation = dataset['validation'].map(\n",
        "    preprocess_function,\n",
        "    batched= True,\n",
        "    remove_columns=dataset[\"validation\"].column_names,\n",
        "    batch_size = 2\n",
        ")\n",
        "\n",
        "tokenized_datasets_test = dataset['test'].map(\n",
        "    preprocess_function,\n",
        "    batched= True,\n",
        "    remove_columns=dataset[\"test\"].column_names,\n",
        "    batch_size = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3BGdRSKMOa4"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3mlvevfMSXh"
      },
      "outputs": [],
      "source": [
        "for parameter in model.parameters():\n",
        "    parameter.requires_grad = True\n",
        "num_layers_to_freeze = 10\n",
        "for layer_index, layer in enumerate(model.model.encoder.layers):\n",
        "    print\n",
        "    if layer_index < len(model.model.encoder.layers) - num_layers_to_freeze:\n",
        "        for parameter in layer.parameters():\n",
        "            parameter.requires_grad = False\n",
        "\n",
        "num_layers_to_freeze = 10\n",
        "for layer_index, layer in enumerate(model.model.decoder.layers):\n",
        "    print\n",
        "    if layer_index < len(model.model.encoder.layers) - num_layers_to_freeze:\n",
        "        for parameter in layer.parameters():\n",
        "            parameter.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIFMq1sBMWFA"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"sacrebleu\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {\"bleu\": result[\"score\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y43IOFvMZ9J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "model.to(device)\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    f\"finetuned-nlp-en-hi\",\n",
        "    gradient_checkpointing=True,\n",
        "    per_device_train_batch_size=32,\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=2,\n",
        "    max_steps=5,\n",
        "    fp16=True,\n",
        "    optim='adafactor',\n",
        "    per_device_eval_batch_size=16,\n",
        "    metric_for_best_model=\"eval_bleu\",\n",
        "    predict_with_generate=True,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "7KH0AZgrMiyF",
        "outputId": "c3eec82a-15af-496d-9c7c-f0a246b96a2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2690316119.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmdosamaspj20010\u001b[0m (\u001b[33mmdosamaspj20010-na\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250813_072946-67a8la1i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mdosamaspj20010-na/huggingface/runs/67a8la1i' target=\"_blank\">glad-energy-2</a></strong> to <a href='https://wandb.ai/mdosamaspj20010-na/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mdosamaspj20010-na/huggingface' target=\"_blank\">https://wandb.ai/mdosamaspj20010-na/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mdosamaspj20010-na/huggingface/runs/67a8la1i' target=\"_blank\">https://wandb.ai/mdosamaspj20010-na/huggingface/runs/67a8la1i</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 04:50, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3909: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5, training_loss=7.763118743896484, metrics={'train_runtime': 365.4703, 'train_samples_per_second': 0.438, 'train_steps_per_second': 0.014, 'total_flos': 3533904543744.0, 'train_loss': 7.763118743896484, 'epoch': 0.06329113924050633})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets_test,\n",
        "    eval_dataset=tokenized_datasets_validation,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "ADaDhU3YUcZq",
        "outputId": "c2a8c56e-7be5-4c37-ceb7-8163bf27e916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a497b34de5d34a2868.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a497b34de5d34a2868.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "def translate(text):\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "  translated_tokens = model.generate(**inputs,  max_length=256)\n",
        "  results = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
        "  return results\n",
        "\n",
        "interface = gr.Interface(fn=translate,inputs=gr.Textbox(lines=2, placeholder='Text to translate'),\n",
        "                        outputs='text')\n",
        "\n",
        "interface.launch()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
